{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T12:25:45.511374Z",
     "start_time": "2019-01-27T12:25:35.278710Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gensim\n",
    "import pyLDAvis\n",
    "import itertools\n",
    "import collections\n",
    "import pandas as pd\n",
    "import pyLDAvis.gensim\n",
    "import tensorflow as tf\n",
    "from ipywidgets import widgets\n",
    "from collections import Counter\n",
    "from IPython.display import display\n",
    "from  tqdm import tqdm, tqdm_notebook\n",
    "from IPython.display import clear_output\n",
    "from itertools import combinations, groupby\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T12:25:45.596347Z",
     "start_time": "2019-01-27T12:25:45.513434Z"
    }
   },
   "outputs": [],
   "source": [
    "########################\n",
    "## OBTAINING PRODUCTS ##\n",
    "########################\n",
    "\n",
    "products = pd.read_csv(\"../0. Data/products.csv\")\n",
    "#order_products_prior = pd.read_csv(\"order_products__prior.csv\")\n",
    "#departments = pd.read_csv(\"departments.csv\")\n",
    "#sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "#orders = pd.read_csv(\"../input/orders.csv\")\n",
    "#aisles = pd.read_csv(\"aisles.csv\")\n",
    "\n",
    "#order_products_train = pd.read_csv(\"../input/order_products__train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T12:25:45.615709Z",
     "start_time": "2019-01-27T12:25:45.599869Z"
    }
   },
   "outputs": [],
   "source": [
    "prod_names = list(products['product_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T12:25:45.945565Z",
     "start_time": "2019-01-27T12:25:45.619459Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                           [chocolate, sandwich, cookies]\n",
       "1                                     [all, seasons, salt]\n",
       "2               [robust, golden, unsweetened, oolong, tea]\n",
       "3        [smart, ones, classic, favorites, mini, rigato...\n",
       "4                           [green, chile, anytime, sauce]\n",
       "5                                         [dry, nose, oil]\n",
       "6                     [pure, coconut, water, with, orange]\n",
       "7                  [cut, russet, potatoes, steam, n, mash]\n",
       "8                   [light, strawberry, blueberry, yogurt]\n",
       "9        [sparkling, orange, juice, prickly, pear, beve...\n",
       "10                                   [peach, mango, juice]\n",
       "11                         [chocolate, fudge, layer, cake]\n",
       "12                                   [saline, nasal, mist]\n",
       "13                     [fresh, scent, dishwasher, cleaner]\n",
       "14                           [overnight, diapers, size, 6]\n",
       "15                      [mint, chocolate, flavored, syrup]\n",
       "16                                   [rendered, duck, fat]\n",
       "17               [pizza, for, one, suprema, frozen, pizza]\n",
       "18       [gluten, free, quinoa, three, cheese, mushroom...\n",
       "19       [pomegranate, cranberry, aloe, vera, enrich, d...\n",
       "20                    [small, medium, dental, dog, treats]\n",
       "21                [fresh, breath, oral, rinse, mild, mint]\n",
       "22                              [organic, turkey, burgers]\n",
       "23       [tri, vi, sol, vitamins, a, c, and, d, supplem...\n",
       "24            [salted, caramel, lean, protein, fiber, bar]\n",
       "25       [fancy, feast, trout, feast, flaked, wet, cat,...\n",
       "26       [complete, spring, water, foaming, antibacteri...\n",
       "27                                   [wheat, chex, cereal]\n",
       "28       [fresh, cut, golden, sweet, no, salt, added, w...\n",
       "29        [three, cheese, ziti, marinara, with, meatballs]\n",
       "                               ...                        \n",
       "49658                          [organic, creamed, coconut]\n",
       "49659                      [professionals, sleek, shampoo]\n",
       "49660                                              [porto]\n",
       "49661                    [bacon, cheddar, pretzel, pieces]\n",
       "49662    [ultra, protein, power, crunch, peanut, butter...\n",
       "49663                  [lemon, cayenne, drinking, vinegar]\n",
       "49664    [super, dark, coconut, ash, banana, chocolate,...\n",
       "49665                   [ginger, snaps, snacking, cookies]\n",
       "49666        [enchilada, with, spanish, rice, beans, meal]\n",
       "49667                  [apple, cinnamon, scented, candles]\n",
       "49668                                [k, cup, dark, blend]\n",
       "49669                              [beef, summer, sausage]\n",
       "49670                             [milk, chocolate, drops]\n",
       "49671                         [cafe, mocha, k, cup, packs]\n",
       "49672         [stone, baked, multi, grain, artisan, rolls]\n",
       "49673       [frozen, greek, yogurt, bars, chocolate, chip]\n",
       "49674           [cinnamon, dolce, keurig, brewed, k, cups]\n",
       "49675                          [ultra, red, energy, drink]\n",
       "49676                  [thick, chunky, sloppy, joe, sauce]\n",
       "49677                   [large, chicken, cheese, taquitos]\n",
       "49678                          [famous, chocolate, wafers]\n",
       "49679             [all, natural, creamy, caesar, dressing]\n",
       "49680       [spaghetti, with, meatballs, and, sauce, meal]\n",
       "49681                                [california, limeade]\n",
       "49682                                    [cucumber, kirby]\n",
       "49683       [vodka, triple, distilled, twist, of, vanilla]\n",
       "49684             [en, croute, roast, hazelnut, cranberry]\n",
       "49685                                  [artisan, baguette]\n",
       "49686    [smartblend, healthy, metabolism, dry, cat, food]\n",
       "49687                           [fresh, foaming, cleanser]\n",
       "Name: product_name, Length: 49688, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products['product_name'] = products['product_name'].str.lower()\n",
    "products['product_name'] = products['product_name'].str.replace('\\W', ' ')\n",
    "products['product_name'] = products['product_name'].str.split()\n",
    "products['product_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W2VEC MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T12:25:48.103515Z",
     "start_time": "2019-01-27T12:25:45.947920Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=10773, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# define training data\n",
    "sentences = list(products['product_name'])\n",
    "# train model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "# summarize the loaded model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T12:25:48.300752Z",
     "start_time": "2019-01-27T12:25:48.105321Z"
    }
   },
   "outputs": [],
   "source": [
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "#print(words)\n",
    "# access vector for one word\n",
    "#print(model['chocolate'])\n",
    "# save model\n",
    "model.save('model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T12:25:48.462992Z",
     "start_time": "2019-01-27T12:25:48.311807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=10773, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "new_model = Word2Vec.load('model.bin')\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T12:25:48.777779Z",
     "start_time": "2019-01-27T12:25:48.465792Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(products['product_name'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T12:26:06.929407Z",
     "start_time": "2019-01-27T12:25:48.780177Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec(documents, vector_size=20, window=4, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T12:26:07.112250Z",
     "start_time": "2019-01-27T12:26:06.931369Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import get_tmpfile\n",
    "fname = get_tmpfile(\"my_doc2vec_model\")\n",
    "model.save(fname)\n",
    "model = Doc2Vec.load(fname)  # you can continue training with the loaded model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T12:26:07.134686Z",
     "start_time": "2019-01-27T12:26:07.114400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00880227,  0.0275582 ,  0.00471736, -0.03012877,  0.00976019,\n",
       "       -0.00742591,  0.00227592,  0.01755355, -0.04901114, -0.05030832,\n",
       "       -0.0231128 , -0.03667214, -0.000898  ,  0.02679087, -0.02006162,\n",
       "        0.01161751,  0.00936987, -0.01719366, -0.03078409,  0.00955646],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = model.infer_vector(list(products['product_name'])[:1][0])\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T12:11:08.383606Z",
     "start_time": "2019-01-27T12:10:59.005295Z"
    }
   },
   "source": [
    "vocab = list(products['product_name'])\n",
    "vectors = list()\n",
    "for i in tqdm(list(products['product_name'])):\n",
    "    vectors.append(model.infer_vector(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T12:26:07.153744Z",
     "start_time": "2019-01-27T12:26:07.142846Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 20)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Important Parameters\n",
    "VOCAB_SIZE = len(prod_names[:1000])#model.wv.vocab)\n",
    "EMBEDDING_DIM = model[\"chocolate\"].shape[0]\n",
    "\n",
    "w2v = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T12:26:07.168743Z",
     "start_time": "2019-01-27T12:26:07.157641Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/gonz/Documents/2Embeddings/tensorboard/metadata.tsv'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsv_file_path = os.getcwd()+\"/tensorboard/metadata.tsv\"\n",
    "tsv_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T12:26:10.768179Z",
     "start_time": "2019-01-27T12:26:07.186599Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b18cbbe19620463f9f9d2bde7b8411c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 20)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(tsv_file_path,'a+', encoding='utf-8') as file_metadata:\n",
    "    for i, word in tqdm_notebook(enumerate(list(products['product_name'])[:VOCAB_SIZE])):\n",
    "        w2v[i] = model.infer_vector(word)\n",
    "        file_metadata.write(list(prod_names[:VOCAB_SIZE])[i] +'\\n')\n",
    "w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T12:26:10.773066Z",
     "start_time": "2019-01-27T12:26:10.769482Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T12:26:10.779561Z",
     "start_time": "2019-01-27T12:26:10.774962Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "TENSORBOARD_FILES_PATH = os.getcwd()+\"/tensorboard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T12:26:10.935075Z",
     "start_time": "2019-01-27T12:26:10.785168Z"
    }
   },
   "outputs": [],
   "source": [
    "#Tensorflow Placeholders\n",
    "X_init = tf.placeholder(tf.float32, shape=(VOCAB_SIZE, EMBEDDING_DIM), name=\"embedding\")\n",
    "X = tf.Variable(X_init)\n",
    "\n",
    "#Initializer\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#Start Tensorflow Session\n",
    "sess = tf.Session()\n",
    "sess.run(init, feed_dict={X_init: w2v})\n",
    "\n",
    "#Instance of Saver, save the graph.\n",
    "saver = tf.train.Saver()\n",
    "writer = tf.summary.FileWriter(TENSORBOARD_FILES_PATH, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T12:26:10.986595Z",
     "start_time": "2019-01-27T12:26:10.939387Z"
    }
   },
   "outputs": [],
   "source": [
    "#Configure a Tensorflow Projector\n",
    "config = projector.ProjectorConfig()\n",
    "embed = config.embeddings.add()\n",
    "embed.metadata_path = tsv_file_path\n",
    "\n",
    "#Write a projector_config\n",
    "projector.visualize_embeddings(writer,config)\n",
    "\n",
    "#save a checkpoint\n",
    "saver.save(sess, TENSORBOARD_FILES_PATH+'/model.ckpt', global_step = VOCAB_SIZE)\n",
    "\n",
    "#close the session\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSNE PAINTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "new_values = tsne_model.fit_transform(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.051756  ,  0.06852241],\n",
       "       [-8.842387  , -1.1197615 ],\n",
       "       [-7.1321006 , -8.833261  ],\n",
       "       ...,\n",
       "       [17.946915  , -2.4528248 ],\n",
       "       [-7.4994574 ,  7.297941  ],\n",
       "       [ 0.33532932,  7.138755  ]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "texts = products['product_name'][:10000]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gonz/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py:1030: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    }
   ],
   "source": [
    "lda = gensim.models.ldamodel.LdaModel(corpus, num_topics=100, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type 'complex' is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-b6784765f941>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlda_vis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_vis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyLDAvis/_display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(data, local, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'd3_url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ldavis_url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ldavis_css_url'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrite_ipynb_local_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepared_data_to_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m def show(data, ip='127.0.0.1', port=8888, n_retries=50,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyLDAvis/_display.py\u001b[0m in \u001b[0;36mprepared_data_to_html\u001b[0;34m(data, d3_url, ldavis_url, ldavis_css_url, template_type, visid, use_http)\u001b[0m\n\u001b[1;32m    176\u001b[0m                            \u001b[0md3_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md3_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                            \u001b[0mldavis_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mldavis_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                            \u001b[0mvis_json\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m                            ldavis_css_url=ldavis_css_url)\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36mto_json\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNumPyEncoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mcheck_circular\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_circular\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_nan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mseparators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseparators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         **kw).encode(obj)\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyLDAvis/utils.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONEncoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[1;32m    179\u001b[0m         raise TypeError(\"Object of type '%s' is not JSON serializable\" %\n\u001b[0;32m--> 180\u001b[0;31m                         o.__class__.__name__)\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type 'complex' is not JSON serializable"
     ]
    }
   ],
   "source": [
    "lda_vis = pyLDAvis.gensim.prepare(lda, corpus, dictionary)\n",
    "pyLDAvis.display(lda_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
